{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saniyayes/AI-Projects/blob/main/House_prediction_kaggle_competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bHlNe8Mlauu"
      },
      "source": [
        "# Project 3: House Price Prediction (Regression) ðŸ \n",
        "\n",
        "**Project Objective:** To build a regression model that accurately predicts the sale price of houses based on a large number of features. This project will cover the complete machine learning workflow, from deep EDA to advanced preprocessing, feature engineering, model training, and evaluation.\n",
        "\n",
        "\n",
        "### Core Concepts:\n",
        "1.  **Regression vs. Classification:** Understanding the goal of predicting a continuous value.\n",
        "2.  **Target Variable Analysis:** Analyzing the distribution of `SalePrice` and applying transformations (log transform).\n",
        "3.  **Advanced Data Preprocessing:** Implementing robust strategies for handling missing values in both numerical and categorical features.\n",
        "4.  **Feature Engineering:** Creating new, powerful features from the existing data to improve model performance.\n",
        "5.  **Categorical Encoding:** Differentiating between and applying Label Encoding and One-Hot Encoding.\n",
        "6.  **Model Building:** Training and comparing a simple baseline model (Linear Regression) with an advanced model (XGBoost).\n",
        "7.  **Model Evaluation:** Understanding and using key regression metrics (RMSE, MAE, R-squared).\n",
        "posch\n",
        "no place not posch\n",
        "which one is costly? based on features  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcoV4KTsMEST",
        "outputId": "5c3aaca1-2375-42b5-c7db-886620beb33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Datasets' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone 'https://github.com/HarshvardhanSingh-13/Datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmUTRw3opAJF"
      },
      "source": [
        "### Step 1: Setup - Importing Libraries and Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W50NNgJAlTcS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd            #regression is for prediction\n",
        "import numpy as np                          #classification is for classifying\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
        "import xgboost as xgb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjEfkMzehYlJ",
        "outputId": "74ae2458-4171-44cb-b71c-9915f9cf9b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Replace YOUR_TOKEN_HERE with the token you got from Kaggle\n",
        "import json\n",
        "import os\n",
        "\n",
        "kaggle_token = {\n",
        "    \"username\": \"san625602\",\n",
        "    \"key\": \"KGAT_5a20c68d1a6efdb2a690c16044bf5cc7\"\n",
        "}\n",
        "\n",
        "# Create .kaggle folder if it doesn't exist\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Save the JSON file\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
        "    json.dump(kaggle_token, f)\n",
        "\n",
        "# Set permissions\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "print(\"kaggle.json created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8GQqruz8h7De",
        "outputId": "8e6291dc-1b12-4f4b-f310-5b394a53ed90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                              title                                                size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------  ---------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "wardabilal/spotify-global-music-dataset-20092025                 Spotify Global Music Dataset (2009â€“2025)          1289021  2025-11-11 09:43:05.933000          16387        409  1.0              \n",
            "neurocipher/heartdisease                                         Heart Disease                                        3491  2025-12-11 15:29:14.327000           2114         68  1.0              \n",
            "prince7489/gaming-hours-vs-academic-and-work-performance         Gaming Hours vs Academic & Work Performance         15335  2025-12-16 15:27:08.140000             40         22  0.9411765        \n",
            "kundanbedmutha/exam-score-prediction-dataset                     Exam Score Prediction Dataset                      325454  2025-11-28 07:29:01.047000           5863        139  1.0              \n",
            "neurocipher/student-performance                                  Student Performance                                 49705  2025-12-12 12:06:28.973000           1261         49  1.0              \n",
            "shaistashahid/child-labor-statistics                             Child Labor Statistics                               5692  2025-12-14 12:50:35.033000            333         23  1.0              \n",
            "oluwatosinadewale/quality-of-life-data                           Work-Life Balance and Longevity Dataset            134618  2025-12-08 07:23:56.853000            138         21  1.0              \n",
            "prince7489/laptop-battery-health-and-usage-dataset               Laptop Battery Health & Usage Dataset                1063  2025-12-15 15:26:25.363000            842         23  0.9411765        \n",
            "ahsanneural/future-jobs-and-skills-demand-2025                   Future Jobs & Skills Demand 2025                   141864  2025-12-09 17:59:20.297000            128         22  0.8235294        \n",
            "khushikyad001/ai-impact-on-jobs-2030                             AI Impact on Jobs 2030                              87410  2025-11-09 17:58:05.410000           9274        209  1.0              \n",
            "rohiteng/amazon-sales-dataset                                    Amazon Sales Dataset                              4037578  2025-11-23 14:29:37.973000           6167         89  1.0              \n",
            "shraddha4ever20/covid-19-patient-symptoms-and-diagnosis-dataset  COVID-19 Patient Symptoms & Diagnosis Dataset        5293  2025-12-13 12:08:43.943000             91         22  0.9411765        \n",
            "siddharth0935/toy-store-e-commerce-database                      Toy Store E-Commerce Database                    16395906  2025-12-15 06:07:31.377000            574         28  1.0              \n",
            "kundanbedmutha/student-performance-dataset                       Student Performance Dataset                        528850  2025-11-17 15:22:39.130000           2873         52  1.0              \n",
            "hetmengar/ola-and-uber-ride-booking-and-cancellation-data        Ola & Uber: Ride Booking & Cancellation Data      3678531  2025-11-23 06:28:52.597000            358         27  1.0              \n",
            "ranaghulamnabi/shopping-behavior-and-preferences-study           Shopping Behavior & Preferences Study               72157  2025-12-03 09:14:26.797000           2201         55  1.0              \n",
            "dhairyajeetsingh/ecommerce-customer-behavior-dataset             Ecommerce Customer Behavior Dataset               2058138  2025-12-13 11:27:03.180000            177         29  1.0              \n",
            "aliiihussain/spotify-analysis-and-visualization                  Spotify Analysis & Visualization                     2369  2025-12-02 19:08:42.523000           1344         33  1.0              \n",
            "aatifahmad123/weather-and-air-quality-of-indian-cities           Weather and Air Quality of Indian Cities             3778  2025-12-14 12:17:28.007000            551         22  1.0              \n",
            "ajinkyachintawar/sales-and-customer-behaviour-insights           Sales & Customer Behaviour Insights                 57108  2025-12-13 09:22:09.340000           1086         35  1.0              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "snmPwGxHsKmw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iwqsJ0iEiFDo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6iTcJ5rdKu"
      },
      "source": [
        "### Step 2: Data Loading via Kaggle API\n",
        "We will load the data directly from the Kaggle competition. This is the standard and most reliable method for using Kaggle datasets in a cloud environment like Colab.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Go to your Kaggle account page ([https://www.kaggle.com/account](https://www.kaggle.com/account)) and click **'Create New Token'** in the API section. This will download a `kaggle.json` file.\n",
        "2.  Run the code cell below. It will prompt you to upload a file. Select the `kaggle.json` file you just downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM8q22forbKk",
        "outputId": "1283be4a-3832-4b01-c168-786209d8f54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json created successfully!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Replace 'YOUR_TOKEN_HERE' with the token you got from Kaggle\n",
        "kaggle_token = {\n",
        "    \"username\": \"San@625602\",\n",
        "    \"key\": \"KGAT_05acd4f031ebf87586fae70c8e586b09\"\n",
        "}\n",
        "\n",
        "# Create .kaggle folder\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Save the JSON file\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
        "    json.dump(kaggle_token, f)\n",
        "\n",
        "# Set permissions\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "print(\"kaggle.json created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnFyyKsysPS0",
        "outputId": "2c6cccc9-85e5-4c6f-a417-1c2d3812f2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/list?page=1\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pM2Cv1EOnMwR"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "# Use Colab's file uploader\n",
        "from google.colab import files\n",
        "print(\"Please upload the kaggle.json file you downloaded from your Kaggle account.\")\n",
        "files.upload()\n",
        "\n",
        "# Create a directory for the Kaggle API configuration\n",
        "!mkdir -p ~/.kaggle\n",
        "# Move the uploaded kaggle.json to the required directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# Set the correct permissions for the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"\\nKaggle API configured successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N-zrCjuxyXU"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c house-prices-advanced-regression-techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyRidxvAxUXY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lljwUR8zlOBN"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufY0ayJEypRx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "kaggle_token = {\n",
        "    \"username\": \"San@625602\",  # not email\n",
        "    \"key\": \"KGAT_7f0885212f24d4fb60498337af7f1eed\"\n",
        "}\n",
        "\n",
        "# Create .kaggle folder\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "\n",
        "# Write kaggle.json\n",
        "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
        "    json.dump(kaggle_token, f)\n",
        "\n",
        "# Set permissions (Linux/Mac)\n",
        "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "\n",
        "print(\"kaggle.json created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkQtnkrjyxC3"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eABTVITsWgF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Unzip the downloaded files\n",
        "!unzip -o house-prices-advanced-regression-techniques.zip\n",
        "\n",
        "print(\"\\nDataset downloaded and unzipped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V6VlTA1tcW8"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaSVBNmatxnj"
      },
      "outputs": [],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62bWYPmbtmu7"
      },
      "outputs": [],
      "source": [
        "train_df.set_index('Id',inplace=True)\n",
        "test_df.set_index('Id',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyW8pK5Dt6WE"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOKqnhPqt7kz"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhKvgLmuYxU"
      },
      "source": [
        "### Step 3: Deep Dive EDA on the Target Variable (`SalePrice`)\n",
        "The most important variable in our dataset is the one we want to predict. Understanding its characteristics is the first and most critical step in any regression problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1Xb0O5iuRKp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(train_df['SalePrice'], kde=True, bins=50)\n",
        "plt.title('Distribution of SalePrice')\n",
        "plt.xlabel('Sale Price')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Skewness of SalePrice: {train_df['SalePrice'].skew()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKcsY3yAvKre"
      },
      "source": [
        "#### **Theoretical Concept: Skewness and Log Transformation**\n",
        "The distribution of `SalePrice` is **positively skewed** (or right-skewed). This means there's a long tail of very expensive houses, which can negatively impact the performance of some models, especially linear models like Linear Regression. These models often assume that the variables (and especially the residuals of the model) are normally distributed.\n",
        "\n",
        "To fix this, we can apply a **log transformation** (`np.log1p`, which is `log(1+x)` to handle potential zero values). This transformation compresses the range of large values, making the distribution more symmetrical and closer to a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-JyDCB7vK4H"
      },
      "outputs": [],
      "source": [
        "print(np.log(10))\n",
        "print(np.log(1000))\n",
        "print(np.log(100000))\n",
        "print(np.log(10000000))\n",
        "print(np.log(100000000))\n",
        "print(np.log(10000000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5p_1qM_vUlE"
      },
      "outputs": [],
      "source": [
        "train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['SalePrice'], kde=True, bins=50, color='green')\n",
        "plt.title('Distribution of Log-Transformed SalePrice')\n",
        "plt.xlabel('Log(Sale Price)')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Skewness of Log-Transformed SalePrice: {train_df['SalePrice'].skew()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yXRfX6xxFI7"
      },
      "source": [
        "**Observation:** After the log transformation, the distribution is much closer to a normal distribution, with skewness close to 0. We will build our model to predict the log of the price, and then convert it back to the original scale for our final predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pPdoh0cxP8D"
      },
      "source": [
        "### Step 4: EDA on Feature Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfJ0YiIMwP3t"
      },
      "outputs": [],
      "source": [
        "# Find the top 10 features most correlated with SalePrice\n",
        "corrmat = train_df.corr(numeric_only=True)\n",
        "top_corr_features = corrmat.nlargest(10, 'SalePrice')['SalePrice'].index\n",
        "top_corr_matrix = train_df[top_corr_features].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(top_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of Top 10 Features with SalePrice')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoKHnxcEx2Jk"
      },
      "source": [
        "**Insight:** The heatmap shows that `OverallQual` (Overall Quality), `GrLivArea` (Above Ground Living Area), and `GarageCars`/`GarageArea` are the most positively correlated features with `SalePrice`. This makes intuitive senseâ€”better quality, larger houses with bigger garages tend to be more expensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW0IojCSyvGK"
      },
      "source": [
        "### Step 5: Data Preprocessing & Feature Engineering\n",
        "This is the most intensive part of the project. We will handle missing values, create new features, and encode categorical variables to prepare the data for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5uaOwg1xg9J"
      },
      "outputs": [],
      "source": [
        "# Combine train and test data for consistent preprocessing\n",
        "all_data = pd.concat((train_df.loc[:,:'SaleCondition'],\n",
        "                      test_df.loc[:,:'SaleCondition']))\n",
        "\n",
        "print(f\"Combined data shape: {all_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA2UZmOzzNH5"
      },
      "source": [
        "Combining the data this way ensures that any preprocessing steps (like handling missing values or encoding categorical features) are applied consistently across both the training and testing datasets, preventing data leakage and potential issues later in the modeling process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET9WKRZFziEy"
      },
      "source": [
        "#### 5.1 Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdQEm8TYzD6n"
      },
      "outputs": [],
      "source": [
        "# Find missing values in the current all_data\n",
        "missing_data = all_data.isna().sum().sort_values(ascending=False)\n",
        "missing_data = missing_data[missing_data > 0]\n",
        "\n",
        "print(\"Features with missing values:\")\n",
        "print(missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmyHZZdv0O_r"
      },
      "outputs": [],
      "source": [
        "# Impute numerical features with 0\n",
        "numerical_cols_to_impute_zero = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageCars', 'GarageArea', 'BsmtFullBath', 'BsmtHalfBath']\n",
        "for col in numerical_cols_to_impute_zero:\n",
        "    if col in all_data.columns:\n",
        "        all_data[col] = all_data[col].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8lUQC470pL7"
      },
      "outputs": [],
      "source": [
        "# Check missing values in numerical columns after imputation with 0\n",
        "all_data[numerical_cols_to_impute_zero].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlASqT4J0zzj"
      },
      "outputs": [],
      "source": [
        "all_data['LotFrontage'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-dvDPFz0t8b"
      },
      "outputs": [],
      "source": [
        "# Impute LotFrontage with the median of the neighborhood\n",
        "if 'LotFrontage' in all_data.columns and all_data['LotFrontage'].isna().any():\n",
        "    all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ZcZpE00-rS"
      },
      "source": [
        "* **all_data.groupby('Neighborhood'):** This groups the DataFrame by the Neighborhood column. The assumption here is that houses in the same neighborhood tend to have similar LotFrontage values.\n",
        "* **.transform(lambda x: x.fillna(x.median())):** This is the core imputation step. For each neighborhood group (x represents the LotFrontage Series for that group), it calculates the median of the existing LotFrontage values in that group (x.median()) and then fills the missing values (x.fillna(...)) within that same group with that calculated median. The transform function ensures that the result has the same index as the original DataFrame, allowing it to be assigned back to the LotFrontage column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK9Af1YQ2dVy"
      },
      "outputs": [],
      "source": [
        "# Find missing values in the current all_data\n",
        "missing_data = all_data.isna().sum().sort_values(ascending=False)\n",
        "missing_data = missing_data[missing_data > 0]\n",
        "\n",
        "print(\"Features with missing values:\")\n",
        "print(missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diRgH4TV08wS"
      },
      "outputs": [],
      "source": [
        "all_data['Alley'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80iDOMHZ3Kil"
      },
      "outputs": [],
      "source": [
        "all_data['Electrical'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chyey67e2W7f"
      },
      "outputs": [],
      "source": [
        "all_data['Fence'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kO1puMU2jVF"
      },
      "outputs": [],
      "source": [
        "# Impute categorical features with 'None' (for features where NA means 'no') or mode (for features where NA means missing data)\n",
        "categorical_cols_to_impute_none = ['Alley', 'Fence', 'MiscFeature', 'PoolQC', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n",
        "for col in categorical_cols_to_impute_none:\n",
        "    if col in all_data.columns: # Check if column exists after one-hot encoding\n",
        "        all_data[col] = all_data[col].fillna('None')\n",
        "\n",
        "for col in ['Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Utilities', 'Functional', 'MSZoning']:\n",
        "    if col in all_data.columns: # Check if column exists after one-hot encoding\n",
        "        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHEPQgJ33Y_p"
      },
      "outputs": [],
      "source": [
        "# Based on the likely remaining missing values (GarageYrBlt), impute the remaining numerical features.\n",
        "# GarageYrBlt can be imputed with 0 (assuming 0 means no garage, consistent with GarageArea/Cars=0)\n",
        "if 'GarageYrBlt' in all_data.columns:\n",
        "    all_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\n",
        "\n",
        "\n",
        "print(\"\\nMissing values after all imputation:\", all_data.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyHnsUW34fqy"
      },
      "source": [
        "#### 5.2 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0Ojb89d3Y7V"
      },
      "outputs": [],
      "source": [
        "# Create a total square footage feature\n",
        "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZICipa3K3Y4s"
      },
      "outputs": [],
      "source": [
        "# Create a total bathrooms feature\n",
        "all_data['TotalBath'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n",
        "                         all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB5WAVOv3Y2b"
      },
      "outputs": [],
      "source": [
        "# Create a feature for age of the house at sale\n",
        "all_data['Age'] = all_data['YrSold'] - all_data['YearBuilt']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW4qG-y_4-y2"
      },
      "source": [
        "#### 5.3 Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnShQOLh5AgU"
      },
      "source": [
        "#### **Theoretical Concept: Ordinal vs. Nominal Features**\n",
        "To use categorical features in a model, we must convert them to numbers. The method depends on the type of feature:\n",
        "1.  **Ordinal Features:** These have an inherent order (e.g., `Poor < Fair < Good < Excellent`). For these, we use **Label Encoding**, which assigns an integer to each category based on its order (e.g., `Poor=0, Fair=1, ...`).\n",
        "2.  **Nominal Features:** These have no inherent order (e.g., `Neighborhood`). Using Label Encoding would imply a false order. Instead, we use **One-Hot Encoding**, which creates a new binary (0/1) column for each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el0sSnl155mq"
      },
      "outputs": [],
      "source": [
        "all_data.select_dtypes(include=['object']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QERlgiyl5qhB"
      },
      "outputs": [],
      "source": [
        "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
        "pd.get_dummies(all_data, columns=categorical_cols, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ5fdjej3YZx"
      },
      "outputs": [],
      "source": [
        "# Convert any remaining object columns to category type for one-hot encoding\n",
        "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
        "all_data = pd.get_dummies(all_data, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "print(f\"Data shape after encoding: {all_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KKhjskJ6owX"
      },
      "source": [
        "### Step 6: Model Building & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3OAbA2q6PdT"
      },
      "outputs": [],
      "source": [
        "# Separate the preprocessed data back into training and testing sets\n",
        "X = all_data[:len(train_df)]\n",
        "y = train_df['SalePrice'] # SalePrice was already log-transformed and is only in train_df\n",
        "X_test_final = all_data[len(train_df):]\n",
        "\n",
        "# Split the training data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # PRNG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqcLUSJE7Sas"
      },
      "source": [
        "#### **Theoretical Concept: Feature Scaling**\n",
        "Many models, especially linear models and distance-based algorithms, perform better when numerical features are on a similar scale. **Standardization** (`StandardScaler`) is a common technique that transforms the data to have a mean of 0 and a standard deviation of 1. This prevents features with large scales (like `GrLivArea`) from dominating features with small scales (like `OverallQual`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HiK6tHm7nm6"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTrAMASq64Vi"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_final_scaled = scaler.transform(X_test_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWf-42lK8Xqh"
      },
      "source": [
        "## **Theoretical Concept: Linear Regression**\n",
        "Linear Regression is a fundamental supervised learning algorithm used for predicting a continuous target variable based on one or more input features. It assumes a linear relationship between the features (independent variables) and the target variable (dependent variable).\n",
        "\n",
        "The goal of Linear Regression is to find the best-fitting straight line (or hyperplane in higher dimensions) that minimizes the sum of the squared differences between the observed and predicted values. This is known as the Ordinary Least Squares (OLS) method.\n",
        "\n",
        "The equation for simple linear regression (one feature) is:\n",
        "$$y = \\beta_0 + \\beta_1x + \\epsilon$$\n",
        "Where:\n",
        "- $y$ is the target variable (SalePrice in our case)\n",
        "- $x$ is the input feature\n",
        "- $\\beta_0$ is the y-intercept\n",
        "- $\\beta_1$ is the coefficient for the feature $x$ (representing the change in $y$ for a one-unit change in $x$)\n",
        "- $\\epsilon$ is the error term\n",
        "\n",
        "For multiple linear regression (multiple features), the equation is:\n",
        "$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$$\n",
        "Where $x_1, x_2, ..., x_n$ are the input features and $\\beta_1, \\beta_2, ..., \\beta_n$ are their respective coefficients.\n",
        "\n",
        "**Assumptions of Linear Regression:**\n",
        "1. **Linearity:** The relationship between the features and the target variable is linear.\n",
        "2. **Independence:** The observations are independent of each other.\n",
        "3. **Homoscedasticity:** The variance of the errors is constant across all levels of the features.\n",
        "4. **Normality:** The errors are normally distributed.\n",
        "5. **No Multicollinearity:** The features are not highly correlated with each other.\n",
        "\n",
        "While Linear Regression is simple and interpretable, it can be sensitive to outliers and may not perform well if the assumptions are violated or if the relationships are highly non-linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAGF7ygP983B"
      },
      "source": [
        "#### 6.1 Model 1: Linear Regression (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvZ6BR5N8X3i"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled,y_train)\n",
        "y_pred_lr = lr.predict(X_val_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ8g4iUe-Tuj"
      },
      "source": [
        "### **Theoretical Concept: XGBoost (Extreme Gradient Boosting)**\n",
        "XGBoost is a highly efficient and popular gradient boosting algorithm. It's an optimized distributed gradient boosting library designed to be highly flexible, portable, and efficient.\n",
        "\n",
        "**How it works:**\n",
        "XGBoost builds trees sequentially. Each new tree attempts to correct the errors made by the previous trees. The predictions from all the trees are then summed up to get the final prediction.\n",
        "\n",
        "**Key Features and Advantages:**\n",
        "1. **Regularization:** Includes L1 and L2 regularization to prevent overfitting.\n",
        "2. **Handling Missing Values:** Has a built-in mechanism to handle missing values.\n",
        "3. **Tree Pruning:** Supports 'depth-first' and 'breadth-first' tree growth and pruning, which can improve performance and reduce overfitting.\n",
        "4. **Parallel Processing:** Designed to be highly parallelizable, making it faster than traditional gradient boosting implementations.\n",
        "5. **Flexibility:** Supports various objective functions and evaluation metrics.\n",
        "\n",
        "XGBoost is known for its performance on structured data and is often a top choice in machine learning competitions. However, it can be more complex to tune and understand compared to simpler models like Linear Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPNaY-pK-v59"
      },
      "source": [
        "#### 6.2 Model 2: XGBoost (Advanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hWusfEp-RUm"
      },
      "outputs": [],
      "source": [
        "xgbr = xgb.XGBRegressor(objective='reg:squarederror', # Corrected objective function\n",
        "                        n_estimators=1000,\n",
        "                        learning_rate=0.05,\n",
        "                        max_depth=3,\n",
        "                        min_child_weight=1,\n",
        "                        subsample=0.8,\n",
        "                        colsample_bytree=0.8,\n",
        "                        random_state=42)\n",
        "\n",
        "# XGBoost can handle NaNs, but since we've cleaned the data, we can use the scaled data as well if preferred.\n",
        "# However, XGBoost generally doesn't require scaling. We'll use the unscaled data as it's a tree-based model.\n",
        "xgbr.fit(X_train, y_train)\n",
        "y_pred_xgb = xgbr.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXzI9VBz_uCZ"
      },
      "source": [
        "### Step 7: Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjMH98TY_uOJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bNwfcxL_vUp"
      },
      "source": [
        "#### **Theoretical Concept: Regression Metrics**\n",
        "- **Mean Absolute Error (MAE):** The average absolute difference between the predicted and actual values. It's easy to interpret.\n",
        "- **Mean Squared Error (MSE):** The average of the squared differences. It penalizes larger errors more heavily.\n",
        "- **Root Mean Squared Error (RMSE):** The square root of MSE. It's the most common metric because it's in the same units as the target variable (in our case, log-price), making it more interpretable than MSE.\n",
        "- **R-squared ($R^2$):** The proportion of the variance in the target variable that is predictable from the features. A value closer to 1 indicates a better fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZM2pGZh_vfx"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"--- {model_name} Performance ---\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE:  {mae:.4f}\")\n",
        "    print(f\"R-squared: {r2:.4f}\\n\")\n",
        "\n",
        "# evaluate_model(y_val, y_pred_lr, \"Linear Regression\") # Commenting out Linear Regression evaluation as it failed\n",
        "\n",
        "evaluate_model(y_val, y_pred_xgb, \"XGBoost\")\n",
        "print('--'*25)\n",
        "evaluate_model(y_val, y_pred_lr, \"LinearRegression\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjqsDDs5AQ-i"
      },
      "source": [
        "### Step 8: Create Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO1J8eGPAG4_"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the final test set using the best model (XGBoost)\n",
        "# Use the unscaled test data for XGBoost prediction\n",
        "final_predictions_log = xgbr.predict(X_test_final)\n",
        "\n",
        "# IMPORTANT: We must reverse the log transformation to get the predictions back on the original price scale\n",
        "final_predictions = np.expm1(final_predictions_log)\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission = pd.DataFrame({'Id': test_df.index, 'SalePrice': final_predictions})\n",
        "\n",
        "# Save to csv\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission.csv' created successfully.\")\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "caDqhCTL7P8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9zropLwAZkh"
      },
      "source": [
        "### Step 9: Conclusion\n",
        "\n",
        "In this project, we successfully built an end-to-end regression pipeline to predict house prices.\n",
        "\n",
        "**Key Steps Undertaken:**\n",
        "1.  **Analyzed and transformed the target variable** (`SalePrice`) to handle its skewness.\n",
        "2.  Conducted a **thorough EDA** to understand the key features influencing price.\n",
        "3.  Implemented a **robust preprocessing strategy**, handling various types of missing data and encoding categorical features correctly.\n",
        "4.  **Engineered new features** (`TotalSF`, `TotalBath`, `Age`) that provided more predictive power.\n",
        "5.  **Trained and compared two models**, demonstrating the superior performance of XGBoost over a simple linear model.\n",
        "6.  **Evaluated the models** using standard regression metrics and generated a final submission file.\n",
        "\n",
        "**Potential Next Steps:**\n",
        "- **Hyperparameter Tuning:** Use techniques like GridSearchCV to find the optimal parameters for the XGBoost model.\n",
        "- **More Feature Engineering:** Create more complex features, such as interaction terms between key variables.\n",
        "- **Ensemble Modeling:** Combine the predictions of several different models to potentially achieve even better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7De0zBzAZvu"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-T7fGz19Jx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "submission = pd.read_csv(\"submission.csv\")\n",
        "\n",
        "# Find missing IDs\n",
        "missing_ids = set(test[\"Id\"]) - set(submission[\"Id\"])\n",
        "print(missing_ids)"
      ],
      "metadata": {
        "id": "TRiRlrnn8tay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: if the missing ID is 1461\n",
        "missing_id = list(missing_ids)[0]\n",
        "submission = submission.append({\"Id\": missing_id, \"SalePrice\": 0}, ignore_index=True)\n",
        "\n",
        "# Save corrected submission\n",
        "submission.to_csv(\"submission_corrected.csv\", index=False)"
      ],
      "metadata": {
        "id": "h1m36MGV804A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use mean SalePrice as fallback\n",
        "mean_price = submission[\"SalePrice\"].mean()\n",
        "\n",
        "for mid in missing_ids:\n",
        "    submission.loc[len(submission)] = [mid, mean_price]\n",
        "\n",
        "# Sort by Id to match Kaggle format\n",
        "submission = submission.sort_values(\"Id\").reset_index(drop=True)\n",
        "\n",
        "print(\"Fixed submission rows:\", len(submission))"
      ],
      "metadata": {
        "id": "30uIV3Bt9EAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission.csv\")"
      ],
      "metadata": {
        "id": "bXVFiJFR9K1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H6hmGns_WJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ydata-profiling"
      ],
      "metadata": {
        "id": "qVoQ38jL2DD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xveMgI6p_0sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport"
      ],
      "metadata": {
        "id": "K9rwfu05fOL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "6u0vu1AS_90Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile_train.to_file(\"train_profile.html\")\n",
        "profile_test.to_file(\"test_profile.html\")"
      ],
      "metadata": {
        "id": "k9yiycuYBBjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJ5omjhLCruj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"train_profile.html\")\n",
        "files.download(\"test_profile.html\")"
      ],
      "metadata": {
        "id": "B7YSBFJzBNQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fXXSi5lDmDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}